% BiBTeX Sample Database

@phdthesis{Brabanter2011,
author = {Brabanter, Kris De},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brabanter - 2011 - Least Squares Support Vector Regression with Applications to Large-Scale Data a Statistical Approach.pdf:pdf},
isbn = {9789460183515},
keywords = {Fixed Size,LS SVM,Non Parametric,Regression},
mendeley-tags = {Fixed Size,LS SVM,Non Parametric,Regression},
number = {April},
school = {KU Leuven},
title = {{Least Squares Support Vector Regression with Applications to Large-Scale Data : a Statistical Approach}},
url = {ftp://ftp.esat.kuleuven.be/pub/SISTA//kdebaban/11-82.pdf},
year = {2011}
}

@article{DeBrabanter2010,
abstract = {A modified active subset selection method based on quadratic R??nyi entropy and a fast cross-validation for fixed-size least squares support vector machines is proposed for classification and regression with optimized tuning process. The kernel bandwidth of the entropy based selection criterion is optimally determined according to the solve-the-equation plug-in method. Also a fast cross-validation method based on a simple updating scheme is developed. The combination of these two techniques is suitable for handling large scale data sets on standard personal computers. Finally, the performance on test data and computational time of this fixed-size method are compared to those for standard support vector machines and ??-support vector machines resulting in sparser models with lower computational cost and comparable accuracy. ?? 2010 Elsevier B.V. All rights reserved.},
author = {{De Brabanter}, K. and {De Brabanter}, J. and Suykens, J. a K and {De Moor}, B.},
doi = {10.1016/j.csda.2010.01.024},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Brabanter et al. - 2010 - Optimized fixed-size kernel models for large data sets.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Classification,Cross-validation,Entropy,Kernel methods,Least squares support vector machines,Plug-in estimate,Regression},
month = jun,
number = {6},
pages = {1484--1504},
publisher = {Elsevier B.V.},
title = {{Optimized fixed-size kernel models for large data sets}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310000393},
volume = {54},
year = {2010}
}

@book{Suykens2002,
author = {Suykens, Johan and Esat-scd-sista, K U Leuven},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suykens, Esat-scd-sista - 2002 - Least Squares Support Vector Machines.pdf:pdf},
isbn = {9812381511},
number = {July},
title = {{Least Squares Support Vector Machines}},
year = {2002}
}

@article{10.1109/MIS.2009.36,
author = {Alon Halevy and Peter Norvig and Fernando Pereira},
title = {The Unreasonable Effectiveness of Data},
journal ={IEEE Intelligent Systems},
volume = {24},
number = {2},
issn = {1541-1672},
year = {2009},
pages = {8-12},
doi = {http://doi.ieeecomputersociety.org/10.1109/MIS.2009.36},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}

@article{Suykens1999,
author = {Suykens, J A K and Vandewalle, J},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suykens, Vandewalle - 1999 - Least Squares Support Vector Machine Classifiers.pdf:pdf},
keywords = {classification,linear least squares,radial basis function,support vector machines},
pages = {1--10},
title = {{Least Squares Support Vector Machine Classifiers}},
year = {1999}
}

@article{Valentini2004,
abstract = {Bias-variance analysis provides a tool to study learning algorithms and can be used to properly design ensemble methods well tuned to the properties of a specific base learner. Indeed the effectiveness of ensemble methods critically depends on accuracy, diversity and learning characteristics of base learners. We present an extended experimental analysis of bias-variance decomposition of the error in Support Vector Machines (SVMs), considering Gaussian, polynomial and dot product kernels. A characterization of the error decomposition is provided, by means of the analysis of the relationships between bias, variance, kernel type and its parameters, offering insights into the way SVMs learn. The results show that the expected trade-off between bias and variance is sometimes observed, but more complex relationships can be detected, especially in Gaussian and polynomial kernels. We show that the bias-variance decomposition offers a rationale to develop ensemble methods using SVMs as base learners, and we outline two directions for developing SVM ensembles, exploiting the SVM bias characteristics and the bias-variance dependence on the kernel parameters.},
author = {Valentini, Giorgio and Dipartimento, D S I and Dietterich, Thomas G},
file = {:home/mandar/Documents/SVM/valentini04a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {bias variance analysis,ensemble methods,multi classifier,support vector machines},
pages = {725--775},
title = {{Bias-Variance Analysis of Support Vector Machines for the Development of SVM-Based Ensemble Methods}},
url = {http://portal.acm.org/citation.cfm?id=1016783},
volume = {5},
year = {2004}
}
