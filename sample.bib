% BiBTeX Sample Database

@phdthesis{Brabanter2011,
author = {De Brabanter, Kris},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brabanter - 2011 - Least Squares Support Vector Regression with Applications to Large-Scale Data a Statistical Approach.pdf:pdf},
isbn = {9789460183515},
keywords = {Fixed Size,LS SVM,Non Parametric,Regression},
mendeley-tags = {Fixed Size,LS SVM,Non Parametric,Regression},
number = {April},
school = {KU Leuven},
title = {{Least Squares Support Vector Regression with Applications to Large-Scale Data : a Statistical Approach}},
url = {ftp://ftp.esat.kuleuven.be/pub/SISTA//kdebaban/11-82.pdf},
year = {2011}
}

@article{DeBrabanter2010,
abstract = {A modified active subset selection method based on quadratic R??nyi entropy and a fast cross-validation for fixed-size least squares support vector machines is proposed for classification and regression with optimized tuning process. The kernel bandwidth of the entropy based selection criterion is optimally determined according to the solve-the-equation plug-in method. Also a fast cross-validation method based on a simple updating scheme is developed. The combination of these two techniques is suitable for handling large scale data sets on standard personal computers. Finally, the performance on test data and computational time of this fixed-size method are compared to those for standard support vector machines and ??-support vector machines resulting in sparser models with lower computational cost and comparable accuracy. ?? 2010 Elsevier B.V. All rights reserved.},
author = {{De Brabanter}, K. and {De Brabanter}, J. and Suykens, J. A. K. and {De Moor}, B.},
doi = {10.1016/j.csda.2010.01.024},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Brabanter et al. - 2010 - Optimized fixed-size kernel models for large data sets.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Classification,Cross-validation,Entropy,Kernel methods,Least squares support vector machines,Plug-in estimate,Regression},
month = jun,
number = {6},
pages = {1484--1504},
publisher = {Elsevier B.V.},
title = {{Optimized fixed-size kernel models for large data sets}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310000393},
volume = {54},
year = {2010}
}

@book{Suykens2002,
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suykens, Esat-scd-sista - 2002 - Least Squares Support Vector Machines.pdf:pdf},
isbn = {9812381511},
  title={Least Squares Support Vector Machines},
  author={Suykens, J.A.K. and Van Gestel, T. and De Brabanter, J. and De Moor, B. and Vandewalle, J.},
  isbn={9789812381514},
  year={2002},
  publisher={World Scientific}
}

@article{10.1109/MIS.2009.36,
author = {Alon Halevy and Peter Norvig and Fernando Pereira},
title = {The Unreasonable Effectiveness of Data},
journal ={IEEE Intelligent Systems},
volume = {24},
number = {2},
issn = {1541-1672},
year = {2009},
pages = {8-12},
doi = {http://doi.ieeecomputersociety.org/10.1109/MIS.2009.36},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}

@article{Suykens1999,
author = {Suykens, J. A. K. and Vandewalle, J},
file = {:home/mandar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suykens, Vandewalle - 1999 - Least Squares Support Vector Machine Classifiers.pdf:pdf},
keywords = {classification,linear least squares,radial basis function,support vector machines},
title = {{Least Squares Support Vector Machine Classifiers}},
year = {1999},
journal={Neural processing letters},
volume={9},
number={3},
pages={293--300},
year={1999},
publisher={Springer}
}

@article{Valentini2004,
abstract = {Bias-variance analysis provides a tool to study learning algorithms and can be used to properly design ensemble methods well tuned to the properties of a specific base learner. Indeed the effectiveness of ensemble methods critically depends on accuracy, diversity and learning characteristics of base learners. We present an extended experimental analysis of bias-variance decomposition of the error in Support Vector Machines (SVMs), considering Gaussian, polynomial and dot product kernels. A characterization of the error decomposition is provided, by means of the analysis of the relationships between bias, variance, kernel type and its parameters, offering insights into the way SVMs learn. The results show that the expected trade-off between bias and variance is sometimes observed, but more complex relationships can be detected, especially in Gaussian and polynomial kernels. We show that the bias-variance decomposition offers a rationale to develop ensemble methods using SVMs as base learners, and we outline two directions for developing SVM ensembles, exploiting the SVM bias characteristics and the bias-variance dependence on the kernel parameters.},
author = {Valentini, Giorgio and Dipartimento, D S I and Dietterich, Thomas G},
file = {:home/mandar/Documents/SVM/valentini04a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {bias variance analysis,ensemble methods,multi classifier,support vector machines},
pages = {725--775},
title = {{Bias-Variance Analysis of Support Vector Machines for the Development of SVM-Based Ensemble Methods}},
url = {http://portal.acm.org/citation.cfm?id=1016783},
volume = {5},
year = {2004}
}

@article{Zaharia2010,
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs; and can be used to interactively query a 39 GB dataset with sub-second response time.; as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals; most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
doi = {10.1007/s00256-009-0861-0},
file = {:home/mandar/Documents/SVM/hotcloud\_spark.pdf:pdf},
issn = {03642348},
journal = {HotCloud'10 Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
pages = {10},
title = {{Spark : Cluster Computing with Working Sets}},
year = {2010}
}

@misc{Spark:2010,
      title  = "Apache Spark: Lightning-fast cluster computing",
      url    = "http://http://spark.apache.org/",
      year   = "2010 (accessed July 6, 2015)"
}

@misc{Hadoop:2005,
      title  = "Apache Hadoop: Lightning-fast cluster computing",
      url    = "http://hadoop.apache.org/",
      year   = "2005 (accessed July 6, 2015)"
}

@misc{Titan:2014,
      title  = "Titan: Distributed Graph Database",
      url    = "http://thinkaurelius.github.io/titan/",
      year   = "2014 (accessed July 6, 2015)"
}

@misc{OrientDB:2010,
      title  = "OrientDB",
      url    = "http://orientdb.com/orientdb/",
      year   = "2010 (accessed July 6, 2015)"
}

@misc{Neo4j:2010,
      title  = "Neo4j: The worlds leading graph database",
      url    = "http://neo4j.com/",
      year   = "2007 (accessed July 6, 2015)"
}

@article{chang2008bigtable,
  title={Bigtable: A distributed storage system for structured data},
  author={Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
  journal={ACM Transactions on Computer Systems (TOCS)},
  volume={26},
  number={2},
  pages={1-26},
  year={2008},
  publisher={ACM}
}

@article{Xavier-De-Souza2010,
abstract = {We present a new class of methods for the global optimization of continuous variables based on simulated annealing (SA). The coupled SA (CSA) class is characterized by a set of parallel SA processes coupled by their acceptance probabilities. The coupling is performed by a term in the acceptance probability function, which is a function of the energies of the current states of all SA processes. A particular CSA instance method is distinguished by the form of its coupling term and acceptance probability. In this paper, we present three CSA instance methods and compare them with the uncoupled case, i.e., multistart SA. The primary objective of the coupling in CSA is to create cooperative behavior via information exchange. This aim helps in the decision of whether uphill moves will be accepted. In addition, coupling can provide information that can be used online to steer the overall optimization process toward the global optimum. We present an example where we use the acceptance temperature to control the variance of the acceptance probabilities with a simple control scheme. This approach leads to much better optimization efficiency, because it reduces the sensitivity of the algorithm to initialization parameters while guiding the optimization process to quasioptimal runs. We present the results of extensive experiments and show that the addition of the coupling and the variance control leads to considerable improvements with respect to the uncoupled case and a more recently proposed distributed version of SA.},
author = {Xavier-De-Souza, Samuel and Suykens, J. A. K. and Vandewalle, J. and Bolle, D\'{e}sir\'{e}},
doi = {10.1109/TSMCB.2009.2020435},
file = {:home/mandar/Documents/SVM/Coupled Simulated Annealing.pdf:pdf},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Cooperative behavior,Distributed optimization,Global optimization,Simulated annealing (SA)},
number = {2},
pages = {320--335},
pmid = {19651558},
title = {{Coupled simulated annealing}},
volume = {40},
year = {2010}
}

@article{Nelder1965,
abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
author = {Nelder, J. A. and Mead, R.},
doi = {10.1093/comjnl/7.4.308},
issn = {0010-4620},
journal = {The Computer Journal},
month = jan,
number = {4},
pages = {308--313},
title = {{A Simplex Method for Function Minimization}},
url = {http://comjnl.oxfordjournals.org/cgi/content/long/7/4/308},
volume = {7},
year = {1965}
}

@article{Borthakur2011,
abstract = {Facebook recently deployed Facebook Messages, its first ever user-facing application built on the Apache Hadoop platform. Apache HBase is a database-like layer built on Hadoop designed to support billions of messages per day. This paper describes the reasons why Facebook chose Hadoop and HBase over other systems such as Apache Cassandra and Voldemort and discusses the application's requirements for consistency, availability, partition tolerance, data model and scalability. We explore the enhancements made to Hadoop to make it a more effective realtime system, the tradeoffs we made while configuring the system, and how this solution has significant advantages over the sharded MySQL database scheme used in other applications at Facebook and many other web-scale companies. We discuss the motivations behind our design choices, the challenges that we face in day-to-day operations, and future capabilities and improvements still under development. We offer these observations on the deployment as a model for other companies who are contemplating a Hadoop-based solution over traditional sharded RDBMS deployments.},
author = {Borthakur, Dhruba and Rash, Samuel and Schmidt, Rodrigo and Aiyer, Amitanand and Gray, Jonathan and Sarma, Joydeep Sen and Muthukkaruppan, Kannan and Spiegelberg, Nicolas and Kuang, Hairong and Ranganathan, Karthik and Molkov, Dmytro and Menon, Aravind},
doi = {10.1145/1989323.1989438},
file = {:home/mandar/Documents/Machine Learning/RealtimeHadoopSigmod2011.pdf:pdf},
isbn = {9781450306614},
issn = {18734235},
journal = {SIGMOD '11 - Proceedings of the 2011 international conference on Management of data},
keywords = {HBase,data,distributed systems,hadoop,hive,scalability,scribe},
pages = {1071},
pmid = {18829293},
title = {{Apache hadoop goes realtime at Facebook}},
url = {http://dl.acm.org/citation.cfm?id=1989323.1989438},
year = {2011}
}

@inproceedings{Mall2013,
author = {Mall, R. and Suykens, J. A. K.},
title = {{Sparse Reductions for Fixed-Size Least Squares Support Vector Machines on Large Scale Data}},
booktitle = {Proc. of 17th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2013)},
pages = {161-173},
year = {2013}
}

@article{Mall2015,
author = {Mall, R. and Suykens, J. A. K.},
title = {{Very Sparse LSSVM Reductions for Large-Scale Data}},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
volume = {26},
number = {5},
pages = {1086-1097},
year = {2015}
}

@book{lssvmbook,
    author    = "Suykens, J. A. K. and Van Gestel, T. and De Brabanter, J. and De Moor, B. and Vandewalle, J.",
    title     = "Least Squares Support Vector Machines",
    year      = "2002",
    publisher = "World Scientific, Singapore",
}

@inproceedings{Mall2014,
author = {Mall, R. and Jumutc, V. and Langone, R. and Suykens, J. A. K.},
title = {Representative Subsets For Big Data Learning using k-{NN} graphs},
booktitle = {Proc. of IEEE BigData},
pages = {37-42},
year = {2014}
}

@article{Mall2013FURS,
author = {Mall, R. and Langone, R. and Suykens, J. A. K.},
title = {{FURS: Fast and Unique Representative Subset selection retaining large scale community structure}},
journal = {Social Network Analysis and Mining},
volume = {3},
number = {4},
pages = {1075-1095},
year = {2013}
}

@article{Meng,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.06807v1},
author = {Xiangrui Meng and
               Joseph K. Bradley and
               Burak Yavuz and
               Evan R. Sparks and
               Shivaram Venkataraman and
               Davies Liu and
               Jeremy Freeman and
               D. B. Tsai and
               Manish Amde and
               Sean Owen and
               Doris Xin and
               Reynold Xin and
               Michael J. Franklin and
               Reza Zadeh and
               Matei Zaharia and
               Ameet Talwalkar},
eprint = {arXiv:1505.06807v1},
file = {:home/mandar/Documents/SVM/MLLib.pdf:pdf},
title = {{MLlib : Machine Learning in Apache Spark}},
journal   = {CoRR},
volume    = {abs/1505.06807},
year      = {2015},
url       = {http://arxiv.org/abs/1505.06807},
}

@TechReport{scala-overview-tech-report,
  author =       {Martin Odersky and al.},
  title =        {An Overview of the Scala Programming Language},
  institution =  {EPFL Lausanne, Switzerland},
  year =         2004,
  number =       {IC/2004/64}
}